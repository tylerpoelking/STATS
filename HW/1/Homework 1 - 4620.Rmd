---
title: "Homwork 1 - Stats 4620"
author: "Tyler Poelking and Karen Somes"
date: "9/4/2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# installing/loading the package:

```

Question 1:

a) Flexible statistical learning methods favor a dataset with a large number of observations, because larger n's minimize the likelihood of overfitting the data. 

b) An less flexible statistical learning method is preferred here, since, a small number of observations increases the likelihood of overfitting the data. A less flexible model will also increase bias, which will protect the model from adhering to meaningless noise in the data. 

c) If the relationship between the predictors and the response is highly non-linear, a flexible statistical learning method is better because more intricate functions of the predictors are required to properly estimate the response, and flexible methods allow for such functions.

d) An inflexible method is better, because a flexible method will capture much of the useless noise in the data, thus causing overfit and poor performance on non-training data.

```{r Question 2, message=FALSE}
#Part A
library(readr)
college <- read_csv("~/Desktop/All Stuff/School Stuff/STATS/Data/College.csv")
fix (college )

```

```{r Question 2 B}
#Part B 
rownames(college)=college[,1]

college =college [,-1]
#fix (college )
```

```{r Question 2 C i-iv}
#Part C

#i.
summary(college)

#ii.
college$Private =as.factor(college$Private)
attach(college)
A = college[,1:10]
pairs(A)
#iii.
#WORKS
plot(Private, Outstate, main="Boxplot Outstate Tuition by Private Status", 
  	xlab="Private", ylab="Outstate")

#iv.
Elite =rep("No",nrow(college ))
Elite [college$Top10perc >50]=" Yes"
college =data.frame(college ,Elite)
college$Elite =as.factor(college$Elite)
summary(college)

boxplot(Outstate~Elite, main="Boxplot Outstate Tuition by Elite Status", 
  	xlab="Elite", ylab="Outstate")
```

```{r Question 2 C v-vi}
#v.
par(mfrow=c(3,2))
hist(college$Grad.Rate)
hist(college$S.F.Ratio)
hist(college$Room.Board)
hist(college$Apps)
hist(college$Personal)
hist(college$Outstate)

#vi.

#Look into other cost variables and how Private status affects 
par(mfrow=c(1,1))
boxplot(Books~Private, main="Estimated Book Cost by Private Status", 
  	xlab="Private", ylab="Book Cost")
boxplot(Room.Board~Private, main="Room and Board Cost by Private Status", 
  	xlab="Private", ylab="Room and Board")
boxplot(Room.Board~Private, main="Room and Board Costs by Private Status", 
  	xlab="Private", ylab="Personal Spend")

#Are private school students getting more for their $?
boxplot(S.F.Ratio~Private, main="Student to Faculty Ratio by Private Status", 
  	xlab="Private", ylab="S.F.Ratio")
boxplot(PhD~Private, main="% Faculty with Ph.D's by Private Status", 
  	xlab="Private", ylab="% Faculty with Ph.D's")
boxplot(Grad.Rate~Private, main="Grad Rate by Private Status", 
  	xlab="Private", ylab="Grad Rate")
boxplot(Expend~Private, main="Instructional Expenditure per Student by Private Status", 
  	xlab="Private", ylab="Instructional Expenditure per Student")

```


Of the initial 10 features forming the scatterplot matrix, continuous features that had moderate to high positive correlation were: Enroll+F.Undergrad, Accept+Enroll, Accept+F.Undergrad, Accept+Apps, Apps+Enroll, Top10perc+Top25perc, and Outstate+Room.Board. Top10perc+F.Undergrad as well as Top25perc+F.Undergrad also seems to have positive correlation, but not as strong. 

My analysis included exploring how whether or not a college is private affects the various types of costs associated with it. Based on the above boxplots, private colleges have higher state tuition than non-private colleges. Student Book Costs have greater variance for private schools but on average students spend slightly less on books. Room and board costs more on average for private schools. And lastly, personal spend is estimated to be higher on average for private schools. 

Private school have a smaller Student to Faculty Ratio, a higher Graduation Rate and a higher Instructional Expenditure per Student amount. However, the average Percent Faculty with Ph.D's is smaller for private schools, which comes as a suprise, since one might assume more elite professors with higher creditials come with a school that costs more money. 


```{r Question 3}

load('~/Desktop/All Stuff/School Stuff/STATS/Data/credit.Rdata')
print(length(newcredit))

#Summary for initial analysis
summary(newcredit)

keeps <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "Balance")
newcreditCont = newcredit[keeps]

#newcredit$Private =as.factor(college$Private)
attach(newcredit)

#Scatterplot variables
pairs(newcreditCont)

```
Of all the continuous variables, Limit and Rating appear to have the strongest (positive) correlation with Balance. Both correlations seem almost equal, which is not a suprise, since Limit and Rating themselves have an extremely high correlation between each other. Income is also highly correlated. My assumption is that we will only need one of these in the model as to avoid multicollinearity. The reltionship appears linear but it may be of a higher or lower degree, we will have to test this. Cards, Age, and Education don't have high correlation, so the degree of information each of these would add to our model stands questionable.


Categorical variables and their affects on Balance are not easily analyized in a scatterplot such as the one above. We are going to construct boxplots, plotting Balance against these each categorical variable, as seen below.  These charts are much more interpretable and make analysis easier. 
```{r Question 3 continued}

#Student
boxplot(Balance~Student, main="Balance by Student", 
  	xlab="Student", ylab="Balance")
#Married
boxplot(Balance~Married, main="Balance by Married", 
  	xlab="Married", ylab="Balance")
#Ethnicity
boxplot(Balance~Ethnicity, main="Balance by Ethnicity", 
  	xlab="Ethnicity", ylab="Balance")

```

The scale of the y-axis on each of these plots is the same, which allows us to compare between plots. Of the three categorical variables charted, the Student has the most prominent difference in meen balance and thus might contribute the most to our model. The mean Balance between ethnicities varries some too, so we may still use Ethnicity. The two box and whiskers in the 'Balance by Married' chart, however, are quite similar, indicating marrital status does not impact Balance.


```{r Question 3 continued 2}

#Rating? or Limit?
lmfit = lm( Balance ~ Rating)
summary(lmfit)

lmfit2 = lm( Balance ~ Limit)
summary(lmfit2)


```
The model using Rating resulted in a smaller Residual standard error and larger Multiple R-squared (though they were both close, of course), so we will stick with that. 
 

```{r Question 3 continued 3}

#Seeing how addition of student affects fit
lmfit3 = lm( Balance ~ Rating+ Student)
summary(lmfit3)

```
Adding student decreased our RSE and increased our R-squared, awesome, definitely going to add to our model. Now that we have the essentials, lets see how the remaining variables effect the simple linear model that models Balance. 

```{r Question 3 continued 4}
#Income
model = lm( Balance ~ Rating+ Student + Income)
print(sprintf('%s : %s' ,'Income', summary(model)$sigma))

#Married
model = lm( Balance ~ Rating+ Student + Married)
print(sprintf('%s : %s' ,'Married', summary(model)$sigma))

#Ethnicity
model = lm( Balance ~ Rating+ Student + Ethnicity)
print(sprintf('%s : %s' ,'Ethnicity', summary(model)$sigma))

#Cards
model = lm( Balance ~ Rating+ Student + Cards)
print(sprintf('%s : %s' ,'Cards', summary(model)$sigma))

#Age
model = lm( Balance ~ Rating+ Student + Age)
print(sprintf('%s : %s' ,'Age', summary(model)$sigma))

#Education
model = lm( Balance ~ Rating+ Education + Age)
print(sprintf('%s : %s' ,'Education', summary(model)$sigma))
```

Income is certainly a must add. From here, we will use an anova table to explore the addition of any other variables. I'll form the anova table in order of the variables above that corresponded to the smallest residual standard error first. 

```{r Question 3 continued 5}

#lmfit5 = lm( Balance ~ I(Rating^.8)+ Student + Income + Age + Ethnicity + Cards + Married + Education)
lmfit4 = lm( Balance ~ Rating+ Student + Income + Age + Ethnicity + Cards + Married + Education)
anova(lmfit4)
```

None of the other variables after Income add significant information to the model according to the F-statistic.  

```{r Question 3 continued 6}
finalModel1 =lm( Balance ~ Rating+ Student + Income + Age)
summary(finalModel1)

finalModel2 =lm( Balance ~ I(Rating^2)+ Student + Income + Age)
summary(finalModel2)


pwrs = c(.4,.5,.6,.7,.8,.9,1.0,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2.0)
#Quotient?
for (i in pwrs){
    #model = lm(Balance ~ poly(Rating, i))
    model = lm(Balance ~ I(Rating^i)+ Student + Income + Age)
    
    print(sprintf('%s : %s' ,i, summary(model)$sigma))
}
```
Adding a second degree polynomial to the continuous variable Rating did not add much information at all. 1.0 also has the lowest residual sum of squares when testing all possible models with the Rating quotient ranging from 0.4-2.0. For sake of simplicity and interpretability, we will keep the degree equal to 1.0. 


```{r Question 3 continues 7}
fits <- fitted(finalModel1)
## calculate the deviance residuals
dev.resids  <- resid(finalModel1)
plot(fits, dev.resids,
     xlab="fitted values", ylab="Pearson residuals" ,main = "Model Resids")
abline(h=0, lty=2)

#QQ to see if residuals follow normal
qqnorm(finalModel1$residuals)
qqline(finalModel1$residuals)
```


The redisuals look good. They follow the normal qqline well, they experience the same variance across all fitted values (homoscedasticity), and they are centered about zero. This indicates a strong linear model that will predict credit balance accurately.


Question 4
The reducible error can be broken down into the variance of the function $f(x)$ and the squared bias of $f(x)$ as follows:  
MSE = $E[(y_0-\hat{f}(x))^2]$  
$= E[(y_0-E[\hat{f}(x)]+E[\hat{f}(x)]-\hat{f}(x))^2]$    
$= E[(y_0-E[\hat{f}(x)])^2+2((y_0-E[\hat{f}(x)])(E[\hat{f}(x)]-\hat{f}(x)))+(E[\hat{f}(x)]-\hat{f}(x))^2]$   
$= E[(y_0-E[\hat{f}(x)])^2] +2E[y_0-E(\hat{f}(x)))(E(\hat{f}(x))-y_0)]+E[(E(\hat{f}(x))-\hat{f}(x)^2]$    
$= E[(y_0-E[\hat{f}(x)])^2] + 2E(y_0-(E(\hat{f}(x)))E(\hat{f}(x)-E(\hat{f}(x)))+E[(E(\hat{f}(x))-\hat{f}(x))^2]$    
$= E[(y_0-E[\hat{f}(x)])^2] + E[(E(\hat{f}(x))-\hat{f}(x))^2]$   
The irreducible error is the variance of the error terms for $f(x)$ and cannot be accounted for in the estimated function. Therefore the MSE is comprised of the variance of the estimated function, the squared bias, and the variance of the error terms.
